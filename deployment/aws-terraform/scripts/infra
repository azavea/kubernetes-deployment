#!/bin/bash

set -e

if [[ -n "${DEBUG}" ]]; then
    set -x
fi

DIR="$(dirname "$0")"
DIR="$(readlink -f $DIR)"

function usage() {
    echo -n \
"Usage: $(basename "$0") COMMAND OPTION[S]
Execute Terraform subcommands with remote state management.
COMMANDS:
    clear       Clear stale modules and remote state
    plan        Initialize and plan infrastructure execution
    apply       Create infrastructure
    connect     Connect existing infrastructure to host machine
    disconnect  Remove cluster from host's .kube/config
    destroy     Remove existing infrastructure
"
}

if [ "${BASH_SOURCE[0]}" = "${0}" ]; then
    BASE_DIR="${DIR}/.."
    if [ "${1:-}" = "--help" ]; then
        usage
        exit 0
    elif [ "${1:-}" = "connect" ]; then
        TERRAFORM_DIR="${BASE_DIR}/0-hardware"
        pushd "${TERRAFORM_DIR}" > /dev/null
        aws eks update-kubeconfig --name $(terraform output -raw cluster_name)
        popd > /dev/null
        exit 0
    elif [ "${1:-}" = "disconnect" ]; then
        TERRAFORM_DIR="${BASE_DIR}/0-hardware"
        pushd "${TERRAFORM_DIR}" > /dev/null
        CLUSTER_ARN=$(terraform output -raw cluster_arn)
        kubectl config unset "clusters.${CLUSTER_ARN}"
        kubectl config unset "contexts.${CLUSTER_ARN}"
        kubectl config unset "users.${CLUSTER_ARN}"
        popd > /dev/null
        exit 0
    else
        if [[ $# -ne 2 ]]; then
            echo "Stage not provided"
            exit 1
        fi
        STAGE=$(echo ${2} | sed 's%^\(.*[^/]\).\?$%\1%')

        echo
        echo "Running infra ${1:-} for stage ${STAGE}..."
        echo "-----------------------------------------------------"
        echo
    fi

    # Check for required variables
    if [ x"${S3_SETTINGS_BUCKET}" == "x" ]; then
        echo -e "ERROR: S3_SETTINGS_BUCKET environment variable is not defined.  Please specify\nand try again.  (Are you running this in the docker-compose environment?)"
        exit 1
    fi

    if [ x"${ENVIRONMENT}" == "x" ]; then
        echo -e "ERROR: ENVIRONMENT environment variable is not defined.  Please specify and try\nagain.  (Are you running this in the docker-compose environment?)"
        exit 1
    fi

    if [ x"${PROJECT_NAME}" == "x" ]; then
        echo -e "ERROR: PROJECT_NAME environment variable is not defined.  Please specify and try\nagain.  (Are you running this in the docker-compose environment?)"
        exit 1
    fi

    pushd "${BASE_DIR}/${STAGE}" > /dev/null
    REGION=${AWS_DEFAULT_REGION:-$(aws configure get region)}

    S3_SETTINGS_KEY=${ENVIRONMENT}
    TERRAFORM_EXTRA_ARGS=${TERRAFORM_EXTRA_ARGS:-}
    TERRAFORM_DESTROY_ARGS=${TERRAFORM_DESTROY_ARGS:-}

    case "${STAGE}" in
        0-hardware)
            EXTRA_VARS=""
            ;;
        1-services)
            EXTRA_VARS="-var-file \"${BASE_DIR}/0-hardware-output.tfvars\""
            ;;
        application/*)
            EXTRA_VARS="-var-file \"${BASE_DIR}/0-hardware-output.tfvars\" \
                        -var-file \"${BASE_DIR}/1-services-output.tfvars\""
            ;;
        *)
            echo "Unrecognized stage!"
            exit 1
            ;;
    esac

    case "${1}" in
        clear)
            rm -rf .terraform terraform.tfstate*
            ;;
        plan)
            aws s3 cp --region ${REGION} "s3://${S3_SETTINGS_BUCKET}/${S3_SETTINGS_KEY}/terraform.tfvars" \
                "../terraform-${ENVIRONMENT}.tfvars"
            terraform init \
                      -upgrade ${TERRAFORM_EXTRA_ARGS} \
                      -backend-config="bucket=${S3_SETTINGS_BUCKET}" \
                      -backend-config="key=${S3_SETTINGS_KEY}/terraform/${STAGE}" \
                      -backend-config="region=${REGION}"
            PLAN_COMMAND="terraform plan \
                                    -var \"aws_region=${REGION}\" \
                                    -var \"environment=${ENVIRONMENT}\" \
                                    -var \"project_prefix=${PROJECT_NAME}\" \
                                    -var-file \"../terraform-${ENVIRONMENT}.tfvars\" \
                                    ${EXTRA_VARS} \
                                    -out=\"azavea-k8s-${ENVIRONMENT}.tfplan\""
            eval ${PLAN_COMMAND}
            ;;
        apply)
            terraform apply "azavea-k8s-${ENVIRONMENT}.tfplan"
            if ! [[ $STAGE =~ application/* ]]; then
                terraform output > ${BASE_DIR}/${STAGE}-output.tfvars
            fi
            ;;
        destroy)
            set -x
            if [[ $STAGE =~ 0-hardware/? ]]; then
                CLUSTER_ARN=$(terraform output -raw cluster_arn)
            fi
            aws s3 cp --region ${REGION} "s3://${S3_SETTINGS_BUCKET}/${S3_SETTINGS_KEY}/terraform.tfvars" \
                "../terraform-${ENVIRONMENT}.tfvars"
            terraform init \
                      -upgrade ${TERRAFORM_EXTRA_ARGS} \
                      -backend-config="bucket=${S3_SETTINGS_BUCKET}" \
                      -backend-config="key=${S3_SETTINGS_KEY}/terraform/${STAGE}" \
                      -backend-config="region=${REGION}"
            PLAN_COMMAND="terraform plan \
                                    -destroy \
                                    ${TERRAFORM_DESTROY_ARGS} \
                                    -var \"aws_region=${REGION}\" \
                                    -var \"environment=${ENVIRONMENT}\" \
                                    -var \"project_prefix=${PROJECT_NAME}\" \
                                    -var-file \"../terraform-${ENVIRONMENT}.tfvars\" \
                                    ${EXTRA_VARS} \
                                    -out=\"azavea-k8s-destroy.tfplan\""
            eval ${PLAN_COMMAND}
            terraform apply \
                      -destroy \
                      azavea-k8s-destroy.tfplan
            if [[ $STAGE =~ 0-hardware/? ]]; then
                kubectl config unset "clusters.${CLUSTER_ARN}"
                kubectl config unset "contexts.${CLUSTER_ARN}"
                kubectl config unset "users.${CLUSTER_ARN}"
            fi
            rm -f azavea-k8s-destroy.tfplan
            ;;
        *)
            echo "ERROR: I don't have support for that subcommand!"
            usage
            exit 1
            ;;
    esac

    popd > /dev/null
fi
